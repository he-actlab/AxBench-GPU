FANN_FLO_2.1
num_layers=3
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 0, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.38164496421813964844e-01) (1, -8.51023137569427490234e-01) (2, 1.27939999103546142578e+00) (3, 7.48724117875099182129e-02) (4, -4.42363947629928588867e-01) (5, 4.46276217699050903320e-01) (6, -7.16641843318939208984e-01) (0, 9.14697069674730300903e-03) (1, -3.48190665245056152344e-01) (2, -3.66943422704935073853e-03) (3, 1.14360123872756958008e-01) (4, -2.23644003272056579590e-02) (5, 1.64848625659942626953e-01) (6, 9.72534790635108947754e-02) (0, -9.29966196417808532715e-03) (1, -4.18216109275817871094e-01) (2, 1.36822906494140625000e+02) (3, -3.26400518417358398438e+00) (4, 2.40071225166320800781e+00) (5, 1.34799426269531250000e+03) (6, -2.94408321380615234375e-01) (0, -8.37996661663055419922e-01) (1, -6.03197097778320312500e-01) (2, -1.33663434982299804688e+01) (3, 1.62762850522994995117e-01) (4, 3.46918255090713500977e-01) (5, 2.90517568588256835938e+00) (6, -1.90048599243164062500e+00) (0, -6.91225707530975341797e-01) (1, -8.67577016353607177734e-01) (2, 1.58924973011016845703e+00) (3, 1.10324017703533172607e-01) (4, -5.83164393901824951172e-01) (5, 7.07519054412841796875e-01) (6, -5.79831302165985107422e-01) (0, -3.03789973258972167969e-01) (1, -4.47438508272171020508e-01) (2, 1.45550549030303955078e+00) (3, -1.47155904769897460938e+00) (4, -4.20105457305908203125e+00) (5, 1.92787289619445800781e+00) (6, -3.20676952600479125977e-01) (0, -3.59070599079132080078e-02) (1, -3.60553473234176635742e-01) (2, -1.76538690924644470215e-01) (3, 1.50685012340545654297e-01) (4, -2.19635087996721267700e-02) (5, 2.31777325272560119629e-01) (6, 4.78819102048873901367e-01) (0, 1.90265327692031860352e-01) (1, 2.78612852096557617188e-01) (2, -1.37913522720336914062e+01) (3, -2.78438949584960937500e+00) (4, 9.02044010162353515625e+00) (5, -6.50990295410156250000e+00) (6, 4.45880293846130371094e-02) (7, 5.03904700279235839844e-01) (8, 3.21479886770248413086e-01) (9, -1.31008768221363425255e-04) (10, 2.67166178673505783081e-02) (11, -1.28953561186790466309e-01) (12, 2.32190806418657302856e-02) (13, -3.15551161766052246094e-01) (14, 1.53863597661256790161e-02) (15, 2.97024566680192947388e-03) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 0, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.31331443786621093750e+00) (1, 5.29515552520751953125e+00) (2, -8.66958045959472656250e+00) (3, 4.75067108869552612305e-01) (4, 4.20293807983398437500e+00) (5, -4.83139371871948242188e+00) (6, -6.39705479145050048828e-01) (0, -7.86594077944755554199e-02) (1, 2.54760701209306716919e-02) (2, -4.97674167156219482422e-01) (3, -1.70445889234542846680e-01) (4, -1.48645201697945594788e-02) (5, -1.80286276340484619141e+00) (6, -1.66302442550659179688e-01) (0, -5.48614300787448883057e-02) (1, -1.14396557211875915527e-01) (2, 5.88172897696495056152e-02) (3, -1.41886994242668151855e-01) (4, -2.98663023859262466431e-02) (5, -1.37182772159576416016e-01) (6, -5.42691230773925781250e-01) (0, -1.89626445770263671875e+01) (1, -2.06403350830078125000e+01) (2, 2.55003128051757812500e+01) (3, -6.80204105377197265625e+00) (4, -1.86585350036621093750e+01) (5, 2.08901710510253906250e+01) (6, -5.85440516471862792969e-01) (0, 4.30575180053710937500e+00) (1, 5.24662303924560546875e+00) (2, -8.65489387512207031250e+00) (3, 5.83568394184112548828e-01) (4, 4.16758489608764648438e+00) (5, -4.89048337936401367188e+00) (6, -7.27939069271087646484e-01) (0, -1.11820526123046875000e+01) (1, -1.27574710845947265625e+01) (2, -5.81496572494506835938e+00) (3, -3.48449993133544921875e+00) (4, -1.09105243682861328125e+01) (5, 7.65443706512451171875e+00) (6, -3.98129433393478393555e-01) (0, 2.58588838577270507812e+00) (1, 1.11747169494628906250e+00) (2, 4.91320800781250000000e+01) (3, 2.22742223739624023438e+00) (4, 3.00455117225646972656e+00) (5, 4.42656326293945312500e+01) (6, -2.81445622444152832031e-01) (0, 1.49666115641593933105e-01) (1, -1.19791969656944274902e-01) (2, -2.11659893393516540527e-01) (3, -2.30598226189613342285e-02) (4, 7.36597776412963867188e-02) (5, -3.00083577632904052734e-01) (6, -4.28336650133132934570e-01) (7, -1.44685821533203125000e+02) (8, 1.72573130112141370773e-03) (9, -3.58658935874700546265e-03) (10, -1.00000000000000000000e+03) (11, -1.44684753417968750000e+02) (12, -1.50000000000000000000e+03) (13, 2.89367828369140625000e+02) (14, 1.92131921648979187012e-02) (15, 5.55997900664806365967e-03) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 0, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.44148981571197509766e-02) (1, -2.78561562299728393555e-01) (2, -3.82012218236923217773e-01) (3, 1.24041615054011344910e-02) (4, -6.54953205958008766174e-03) (5, 4.56248670816421508789e-01) (6, -1.58651903271675109863e-01) (0, -3.73871847987174987793e-02) (1, -3.78715634346008300781e-01) (2, 3.41687113046646118164e-01) (3, -7.40269124507904052734e-02) (4, -1.90859854221343994141e-01) (5, 2.77594923973083496094e-01) (6, -1.14711415767669677734e+00) (0, -1.15239769220352172852e-01) (1, -5.74099600315093994141e-01) (2, -3.94576162099838256836e-01) (3, -5.32548546791076660156e-01) (4, -8.60709786415100097656e-01) (5, -3.60398826599121093750e+01) (6, -4.93897544220089912415e-03) (0, 1.46439243108034133911e-02) (1, 3.77654373645782470703e-01) (2, -6.50687158107757568359e-01) (3, 1.32060158252716064453e+00) (4, -1.44353752136230468750e+01) (5, -6.27435684204101562500e-01) (6, 1.28402411937713623047e+00) (0, 3.68405967950820922852e-01) (1, -4.10932630300521850586e-01) (2, -5.73079705238342285156e-01) (3, -4.58308249711990356445e-01) (4, -4.03093665838241577148e-01) (5, 9.26148533821105957031e-01) (6, -8.02798628807067871094e-01) (0, 1.02007938548922538757e-02) (1, 3.76929119229316711426e-02) (2, 6.74673914909362792969e-01) (3, -9.37666967511177062988e-02) (4, -3.22939068078994750977e-01) (5, -3.36508929729461669922e-01) (6, -2.29794681072235107422e-02) (0, 1.27820003032684326172e+00) (1, -2.07176423072814941406e+00) (2, -1.58044850826263427734e+00) (3, -1.35092151165008544922e+00) (4, 6.69933557510375976562e-01) (5, 1.99277237057685852051e-01) (6, -2.30099987983703613281e+00) (0, -1.66474094390869140625e+01) (1, -9.54444885253906250000e-01) (2, -1.01521015167236328125e+00) (3, -8.66322040557861328125e-01) (4, 1.98926472663879394531e+00) (5, -1.42875671386718750000e+00) (6, -6.72095000743865966797e-01) (7, -9.58638340234756469727e-02) (8, 6.45742356777191162109e-01) (9, 4.76756319403648376465e-02) (10, 3.33694219589233398438e+00) (11, -8.05796906352043151855e-02) (12, -6.29881024360656738281e-02) (13, -3.01770418882369995117e-02) (14, -9.27583503723144531250e+00) (15, 2.60917656123638153076e-02) 

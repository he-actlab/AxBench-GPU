FANN_FLO_2.1
num_layers=3
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 0, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.01331416517496109009e-02) (1, -4.69337522983551025391e-01) (2, 4.26115095615386962891e-01) (3, 1.15698203444480895996e-03) (4, 6.46742105484008789062e-01) (5, 6.08448311686515808105e-02) (6, 2.99165517091751098633e-01) (0, -4.25967931747436523438e+00) (1, -1.39107170104980468750e+01) (2, 1.84951095581054687500e+01) (3, -1.81888806819915771484e+00) (4, -1.00914108753204345703e+00) (5, -3.36986989714205265045e-03) (6, -7.21241176128387451172e-01) (0, -1.33660280704498291016e+00) (1, -8.89801025390625000000e+01) (2, 1.95854606628417968750e+01) (3, -1.34339541196823120117e-01) (4, -9.05671060085296630859e-01) (5, 1.00965893268585205078e+00) (6, -3.91274169087409973145e-02) (0, -1.11639566421508789062e+01) (1, -2.20236225128173828125e+01) (2, -1.10440998077392578125e+01) (3, -1.54566571116447448730e-01) (4, -6.05041742324829101562e-01) (5, 2.83783316612243652344e+00) (6, -1.14328300952911376953e+00) (0, -5.31133674085140228271e-02) (1, -1.22581414878368377686e-01) (2, -7.15599179267883300781e-01) (3, 1.32015258073806762695e-01) (4, 5.44465065002441406250e-01) (5, -7.15107563883066177368e-03) (6, 3.85929852724075317383e-01) (0, -2.23001375794410705566e-01) (1, 2.25968480110168457031e-01) (2, 6.49157702922821044922e-01) (3, -2.89727747440338134766e-01) (4, -1.32967054843902587891e-01) (5, -1.79230645298957824707e-01) (6, -2.12171986699104309082e-01) (0, -9.52885270118713378906e-01) (1, 1.40049302577972412109e+00) (2, -2.19438171386718750000e+00) (3, 7.18772292137145996094e-01) (4, -1.76827466487884521484e+00) (5, -1.23891317844390869141e+00) (6, -1.18823814392089843750e+00) (0, -9.68226909637451171875e-01) (1, 1.36461532115936279297e+00) (2, -1.96936476230621337891e+00) (3, 9.34605598449707031250e-01) (4, -1.74746954441070556641e+00) (5, -9.64840650558471679688e-01) (6, -1.18257117271423339844e+00) (7, -3.73228117823600769043e-02) (8, 8.00243675708770751953e-01) (9, 1.50000000000000000000e+03) (10, 3.27371765136718750000e+02) (11, -1.19385778903961181641e-01) (12, -1.29389569163322448730e-01) (13, -4.15431499481201171875e+00) (14, 2.61387920379638671875e+00) (15, 1.52376830577850341797e-01) 

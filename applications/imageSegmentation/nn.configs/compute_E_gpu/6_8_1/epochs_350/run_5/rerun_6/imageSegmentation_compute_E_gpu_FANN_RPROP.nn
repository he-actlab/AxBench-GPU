FANN_FLO_2.1
num_layers=3
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 0, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.54850927740335464478e-02) (1, -2.59108394384384155273e-01) (2, 1.11345005035400390625e+00) (3, 4.06664833426475524902e-02) (4, -7.17541038990020751953e-01) (5, -2.98126369714736938477e-01) (6, -1.41261279582977294922e-01) (0, 2.12547749280929565430e-01) (1, 1.24529731273651123047e+00) (2, 1.34452557563781738281e+00) (3, -6.00661814212799072266e-01) (4, 3.51491332054138183594e-01) (5, -2.20962452888488769531e+00) (6, -3.31661367416381835938e+00) (0, -1.63643813133239746094e+00) (1, 1.48871088027954101562e+00) (2, 4.69738101959228515625e+00) (3, -1.67506027221679687500e+00) (4, -5.17886698246002197266e-01) (5, -1.79000532627105712891e+00) (6, 5.13859450817108154297e-01) (0, 1.12143065780401229858e-02) (1, -7.03146904706954956055e-02) (2, 4.66822572052478790283e-02) (3, 5.67187704145908355713e-02) (4, -3.70040953159332275391e-01) (5, 2.41454645991325378418e-01) (6, 1.38111031055450439453e+00) (0, -6.14117741584777832031e-01) (1, 1.23025810718536376953e+00) (2, 1.78290927410125732422e+00) (3, -1.07699751853942871094e+00) (4, -2.43418570607900619507e-02) (5, -2.29884853363037109375e+01) (6, -7.05161213874816894531e-01) (0, -1.49371997070312500000e+03) (1, -4.56501293182373046875e+00) (2, 6.50309085845947265625e+00) (3, -5.30883369445800781250e+01) (4, -7.68093967437744140625e+00) (5, -1.46959997558593750000e+03) (6, -5.37504971027374267578e-01) (0, 1.09970252960920333862e-02) (1, 4.75004166364669799805e-01) (2, -4.64298456907272338867e-01) (3, -3.90657126903533935547e-01) (4, -3.65420281887054443359e-01) (5, 3.81965458393096923828e-01) (6, 4.50726807117462158203e-01) (0, -2.11856383830308914185e-02) (1, -8.79110932350158691406e-01) (2, -3.44510555267333984375e-01) (3, 1.02427154779434204102e-01) (4, 1.67666360735893249512e-01) (5, 5.75016796588897705078e-01) (6, 2.01927185058593750000e+00) (7, -2.17206209897994995117e-01) (8, -1.33084818720817565918e-01) (9, -9.19718742370605468750e-02) (10, 3.60884398221969604492e-01) (11, 1.53459683060646057129e-01) (12, 1.50000000000000000000e+03) (13, -1.63860440254211425781e-01) (14, -3.63888770341873168945e-01) (15, 1.50655493140220642090e-01) 

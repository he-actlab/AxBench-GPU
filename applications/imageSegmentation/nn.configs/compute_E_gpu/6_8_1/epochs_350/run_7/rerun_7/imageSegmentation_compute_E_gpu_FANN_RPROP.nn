FANN_FLO_2.1
num_layers=3
learning_rate=0.500000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 9 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (7, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) (9, 0, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 8.80539789795875549316e-02) (1, 7.98973366618156433105e-02) (2, 4.49807256460189819336e-01) (3, -1.65027007460594177246e-01) (4, -2.38711372017860412598e-01) (5, -1.36448636651039123535e-01) (6, -1.04283106327056884766e+00) (0, 9.26251232624053955078e-01) (1, -5.74868381023406982422e-01) (2, -5.96372708678245544434e-02) (3, -1.37615954875946044922e+00) (4, 1.92594277858734130859e+00) (5, -4.24059391021728515625e+00) (6, -1.51899647712707519531e+00) (0, 8.47016796469688415527e-02) (1, -1.56115412712097167969e-01) (2, 4.11740243434906005859e-01) (3, -1.36993117630481719971e-02) (4, -1.38678625226020812988e-01) (5, -1.40444561839103698730e-01) (6, -2.67021209001541137695e-01) (0, 7.49441757798194885254e-02) (1, -5.27177333831787109375e-01) (2, 5.25779902935028076172e-01) (3, -4.84943874180316925049e-02) (4, -4.36964690685272216797e-01) (5, 4.55000102519989013672e-02) (6, 3.79594899713993072510e-02) (0, -6.94641208648681640625e+00) (1, -6.13255233764648437500e+01) (2, -1.53671354055404663086e-01) (3, -2.30946755409240722656e+00) (4, -4.39729095458984375000e+02) (5, -1.73292398452758789062e-01) (6, -1.70103740692138671875e+00) (0, -6.81186690926551818848e-02) (1, -7.87650465965270996094e-01) (2, 2.01510357856750488281e+00) (3, -2.71622270345687866211e-01) (4, 1.79951179027557373047e+00) (5, 5.04713475704193115234e-01) (6, 2.20741126686334609985e-02) (0, 3.13747429847717285156e+00) (1, 3.51414375305175781250e+01) (2, -9.76024246215820312500e+00) (3, -1.41972172260284423828e+00) (4, -4.12071929931640625000e+02) (5, -1.20903646945953369141e+00) (6, -1.67036771774291992188e+00) (0, -1.08761489391326904297e-01) (1, -7.46520757675170898438e-01) (2, 1.70240804553031921387e-01) (3, -2.55670398473739624023e-01) (4, 2.61301898956298828125e+00) (5, 9.01434719562530517578e-01) (6, 1.05253922939300537109e+00) (7, -1.04375742375850677490e-01) (8, 9.82349924743175506592e-03) (9, 1.04883417487144470215e-01) (10, -8.44621658325195312500e-02) (11, 1.50000000000000000000e+03) (12, -6.09154775738716125488e-02) (13, 6.90725097656250000000e+01) (14, -6.39390051364898681641e-02) (15, 1.25354737043380737305e-01) 
